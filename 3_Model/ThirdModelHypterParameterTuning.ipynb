{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ff06e3672e675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68649d1d5e5e2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(folder):\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".png\") and \"imgs\" in root:\n",
    "                filenames.append(os.path.join(root, file))\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eefa26bf5b3ca3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data/train/imgs/HIP_0094.png', 'data/train/targets/HIP_0094.png')\n",
      "Number of training samples:  320 \n",
      "Number of validation samples:  91 \n",
      "Number of test samples:  47\n"
     ]
    }
   ],
   "source": [
    "files = get_files(\"data/\")\n",
    "train_paths = [(path, path.replace(\"imgs\", \"targets\")) for path in files if \"train\" in path]\n",
    "test_paths = [(path, path.replace(\"imgs\", \"targets\")) for path in files if \"test\" in path]\n",
    "val_paths = [(path, path.replace(\"imgs\", \"targets\")) for path in files if \"valid\" in path]\n",
    "\n",
    "print(train_paths[0])\n",
    "\n",
    "print(\"Number of training samples: \", len(train_paths), \n",
    "      \"\\nNumber of validation samples: \", len(val_paths),\n",
    "      \"\\nNumber of test samples: \", len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ecfbc17c5cb479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    image = tf.cast(image, tf.float32) - tf.reduce_mean(image)\n",
    "    image = image / tf.math.reduce_std(image)\n",
    "    return image\n",
    "\n",
    "def normalize_mask(mask):\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = mask / 255.0\n",
    "    return mask\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=1)\n",
    "    image = tf.image.resize(image, IMSIZE)\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_mask(mask_path):\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=1)\n",
    "    mask = tf.image.resize(mask, IMSIZE)\n",
    "    return mask\n",
    "\n",
    "def load_image_pair(image_path):\n",
    "    image = load_image(image_path[0])\n",
    "    mask = load_mask(image_path[1])\n",
    "    image = normalize_image(image)\n",
    "    mask = normalize_mask(mask)\n",
    "    return image, mask\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "@tf.function\n",
    "def dice(y_true, y_pred):\n",
    "    \"\"\"Computes the Dice loss value between `y_true` and `y_pred`.\n",
    "\n",
    "    Formula:\n",
    "    ```python\n",
    "    loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred))\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        y_true: tensor of true targets.\n",
    "        y_pred: tensor of predicted targets.\n",
    "\n",
    "    Returns:\n",
    "        Dice loss value.\n",
    "    \"\"\"\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    inputs = K.flatten(y_true)\n",
    "    targets = K.flatten(y_pred)\n",
    "    \n",
    "    intersection = K.sum(inputs * targets)\n",
    "    dice = tf.divide(\n",
    "        2.0 * intersection,\n",
    "        K.sum(y_true) + K.sum(y_pred) + K.epsilon(),\n",
    "    )\n",
    "\n",
    "    return 1 - dice\n",
    "\n",
    "@tf.function\n",
    "def dice_metric(y_true, y_pred):\n",
    "    \"\"\"Computes the Dice loss value between `y_true` and `y_pred`.\n",
    "\n",
    "    Formula:\n",
    "    ```python\n",
    "    loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred))\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        y_true: tensor of true targets.\n",
    "        y_pred: tensor of predicted targets.\n",
    "\n",
    "    Returns:\n",
    "        Dice loss value.\n",
    "    \"\"\"\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    inputs = K.flatten(y_true)\n",
    "    targets = K.flatten(y_pred)\n",
    "\n",
    "    intersection = K.sum(inputs * targets)\n",
    "    dice = tf.divide(\n",
    "        2.0 * intersection,\n",
    "        K.sum(y_true) + K.sum(y_pred) + K.epsilon(),\n",
    "    )\n",
    "\n",
    "    return dice\n",
    "\n",
    "class Augment(tf.keras.layers.Layer):\n",
    "    def __init__(self, seed=42):\n",
    "        super().__init__()\n",
    "        # both use the same seed, so they'll make the same random changes.\n",
    "        self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "        self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "        self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"vertical\", seed=seed)\n",
    "        self.augment_labels = tf.keras.layers.RandomFlip(mode=\"vertical\", seed=seed)\n",
    "\n",
    "    def call(self, inputs, labels):\n",
    "        inputs = self.augment_inputs(inputs)\n",
    "        labels = self.augment_labels(labels)\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13a28e25037d8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "BUFFER_SIZE = 100\n",
    "BATCH_SIZE = 16\n",
    "IMSIZE = (256, 256)\n",
    "\n",
    "def create_dataset(paths):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    dataset = dataset.map(load_image_pair, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    batches = (\n",
    "        dataset\n",
    "        .cache()\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        # .map(Augment(), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    return batches\n",
    "\n",
    "train_batches = create_dataset(train_paths)\n",
    "test_batches = create_dataset(test_paths)\n",
    "val_batches = create_dataset(val_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48d0a74fa1dba97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "default_args = {\n",
    "    \"kernel_initializer\": \"he_normal\",\n",
    "    \"padding\": \"same\",\n",
    "    \"activation\": \"relu\"\n",
    "}\n",
    "\n",
    "def unet(dropout=0.2, lr=1e-3, init=\"he_normal\", adam_beta1=0.9, adam_beta2=0.999):\n",
    "    in1 = Input(shape=(*IMSIZE, 1))\n",
    "\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer=init, padding='same')(in1)\n",
    "    conv1 = Dropout(dropout)(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer=init, padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer=init, padding='same')(pool1)\n",
    "    conv2 = Dropout(dropout)(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer=init, padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', kernel_initializer=init, padding='same')(pool2)\n",
    "    conv3 = Dropout(dropout)(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', kernel_initializer=init, padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer=init, padding='same')(pool3)\n",
    "    conv4 = Dropout(dropout)(conv4)\n",
    "    conv4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer=init, padding='same')(conv4)\n",
    "\n",
    "    up1 = concatenate([UpSampling2D((2, 2))(conv4), conv3], axis=-1)\n",
    "    conv5 = Conv2D(64, (3, 3), activation='relu', kernel_initializer=init, padding='same')(up1)\n",
    "    conv5 = Dropout(dropout)(conv5)\n",
    "    conv5 = Conv2D(64, (3, 3), activation='relu', kernel_initializer=init, padding='same')(conv5)\n",
    "\n",
    "    up2 = concatenate([UpSampling2D((2, 2))(conv5), conv2], axis=-1)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', kernel_initializer=init, padding='same')(up2)\n",
    "    conv6 = Dropout(dropout)(conv6)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', kernel_initializer=init, padding='same')(conv6)\n",
    "\n",
    "    up2 = concatenate([UpSampling2D((2, 2))(conv6), conv1], axis=-1)\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', kernel_initializer=init, padding='same')(up2)\n",
    "    conv7 = Dropout(dropout)(conv7)\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', kernel_initializer=init, padding='same')(conv7)\n",
    "    segmentation = Conv2D(1, (1, 1), activation='sigmoid', name='seg')(conv7)\n",
    "\n",
    "    model = Model(inputs=[in1], outputs=[segmentation])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=adam_beta1, beta_2=adam_beta2),\n",
    "                  loss = dice,\n",
    "                  metrics=['accuracy', dice_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c1afc9049840fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = get_second_model(dropout=0.3, lr=4e-4, init=\"he_normal\")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a418ddf18d4bf6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(train_batches,\n",
    "#                    validation_data=val_batches,\n",
    "#                    epochs=150,\n",
    "#                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8d54c745b316a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, masks in test_batches.take(1):\n",
    "#     predictions = model.predict(images)\n",
    "#     fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "#     for ii, (img, msk, prediction) in enumerate(zip(images, masks, predictions)):\n",
    "#         axs[ii, 0].imshow(img, cmap='gray')\n",
    "#         axs[ii, 1].imshow(msk, cmap='gray')\n",
    "#         axs[ii, 2].imshow(prediction, cmap='gray')\n",
    "#         axs[ii, 0].axis(\"off\")\n",
    "#         axs[ii, 1].axis(\"off\")\n",
    "#         axs[ii, 2].axis(\"off\")\n",
    "#         axs[ii, 0].set_title(\"Image\")\n",
    "#         axs[ii, 1].set_title(\"Mask\")\n",
    "#         axs[ii, 2].set_title(\"Prediction\")\n",
    "#         if ii == 2:\n",
    "#             break\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2)\n",
    "# axs[0].plot(history.history['loss'])\n",
    "# axs[0].plot(history.history['val_loss'])\n",
    "# axs[1].plot(history.history['dice_metric'])\n",
    "# axs[1].plot(history.history['val_dice_metric'])\n",
    "# axs[0].set_title('Model loss')\n",
    "# axs[0].set_ylabel('Loss')\n",
    "# axs[0].set_xlabel('Epoch')\n",
    "# axs[0].legend(['Train', 'Val'], loc='upper left')\n",
    "# axs[1].set_title('Model accuracy')\n",
    "# axs[1].set_ylabel('Accuracy')\n",
    "# axs[1].set_xlabel('Epoch')\n",
    "# axs[1].legend(['Train', 'Val'], loc='lower right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdb1e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={\n",
    "    'init': ['he_normal', 'glorot_uniform'],\n",
    "    'dropout_rate': [0.2, 0.3, 0.5],\n",
    "    'adam_beta1': [i / 10 for i in range(1,10)],\n",
    "    'adam_beta2': [0.001 + i / 10 for i in range(1,10)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6157acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# def create_model(lr=0.001, init='he_normal', dropout_rate= 0.1, adam_beta1=0.9, adam_beta2=0.999):\n",
    "#     return KerasClassifier(\n",
    "#         build_fn=get_second_model, \n",
    "#         lr=lr,\n",
    "#         init=init, \n",
    "#         dropout_rate=dropout_rate,\n",
    "#         adam_beta1=adam_beta1,\n",
    "#         adam_beta2=adam_beta2,\n",
    "#         epochs=150, \n",
    "#         batch_size=32, \n",
    "#         verbose=0#\n",
    "#     )\n",
    "\n",
    "# random_search = RandomizedSearchCV(estimator=get_second_model(), param_distributions=parameters, n_iter=10, cv=3, verbose=1)\n",
    "# x_train = np.concatenate([x for x, _ in train_batches], axis=0)\n",
    "# y_train = np.concatenate([y for _, y in train_batches], axis=0)\n",
    "# for x, y in train_batches:\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "#     break\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "\n",
    "# random_search_result = random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eba7290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "determine_learning_rate = False\n",
    "learning_rate_results = {}\n",
    "\n",
    "if determine_learning_rate:\n",
    "    # determine learning rate\n",
    "    for learning_rate in [10 ** -i for i in range(3,6)]:\n",
    "        model = unet()\n",
    "        history = model.fit(train_batches,\n",
    "                        validation_data=val_batches,\n",
    "                        epochs=50,\n",
    "                        verbose=2)\n",
    "        learning_rate_results[learning_rate] = max(history.history['val_dice_metric'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cb74a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e5afcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1c55200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('init', ['he_normal', 'glorot_uniform'])\n",
      "('dropout_rate', [0.2, 0.3, 0.5])\n",
      "('adam_beta1', [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
      "('adam_beta2', [0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.009999999999999998, 0.011, 0.012, 0.013000000000000001, 0.013999999999999999, 0.015, 0.016, 0.017, 0.018000000000000002, 0.019, 0.02, 0.021, 0.022000000000000002, 0.023, 0.024, 0.025, 0.026000000000000002, 0.027, 0.028, 0.029, 0.030000000000000002, 0.031, 0.032, 0.033, 0.034, 0.035, 0.036000000000000004, 0.037, 0.038, 0.039, 0.04, 0.041, 0.042, 0.043000000000000003, 0.044, 0.045, 0.046, 0.047, 0.048, 0.049, 0.05, 0.051000000000000004, 0.052, 0.053, 0.054, 0.055, 0.056, 0.057, 0.058, 0.059000000000000004, 0.06, 0.061, 0.062, 0.063, 0.064, 0.065, 0.066, 0.067, 0.068, 0.069, 0.07, 0.07100000000000001, 0.072, 0.073, 0.074, 0.075, 0.076, 0.077, 0.078, 0.079, 0.08, 0.081, 0.082, 0.083, 0.084, 0.085, 0.08600000000000001, 0.087, 0.088, 0.089, 0.09, 0.091, 0.092, 0.093, 0.094, 0.095, 0.096, 0.097, 0.098, 0.099, 0.1, 0.101, 0.10200000000000001, 0.103, 0.104, 0.105, 0.106, 0.107, 0.108, 0.109, 0.11, 0.111, 0.112, 0.113, 0.114, 0.115, 0.116, 0.117, 0.11800000000000001, 0.119, 0.12, 0.121, 0.122, 0.123, 0.124, 0.125, 0.126, 0.127, 0.128, 0.129, 0.13, 0.131, 0.132, 0.133, 0.134, 0.135, 0.136, 0.137, 0.138, 0.139, 0.14, 0.14100000000000001, 0.142, 0.143, 0.144, 0.145, 0.146, 0.147, 0.148, 0.149, 0.15, 0.151, 0.152, 0.153, 0.154, 0.155, 0.156, 0.157, 0.158, 0.159, 0.16, 0.161, 0.162, 0.163, 0.164, 0.165, 0.166, 0.167, 0.168, 0.169, 0.17, 0.171, 0.17200000000000001, 0.173, 0.174, 0.175, 0.176, 0.177, 0.178, 0.179, 0.18, 0.181, 0.182, 0.183, 0.184, 0.185, 0.186, 0.187, 0.188, 0.189, 0.19, 0.191, 0.192, 0.193, 0.194, 0.195, 0.196, 0.197, 0.198, 0.199, 0.2, 0.201, 0.202, 0.203, 0.20400000000000001, 0.205, 0.206, 0.207, 0.208, 0.209, 0.21, 0.211, 0.212, 0.213, 0.214, 0.215, 0.216, 0.217, 0.218, 0.219, 0.22, 0.221, 0.222, 0.223, 0.224, 0.225, 0.226, 0.227, 0.228, 0.229, 0.23, 0.231, 0.232, 0.233, 0.234, 0.23500000000000001, 0.236, 0.237, 0.238, 0.239, 0.24, 0.241, 0.242, 0.243, 0.244, 0.245, 0.246, 0.247, 0.248, 0.249, 0.25, 0.251, 0.252, 0.253, 0.254, 0.255, 0.256, 0.257, 0.258, 0.259, 0.26, 0.261, 0.262, 0.263, 0.264, 0.265, 0.266, 0.267, 0.268, 0.269, 0.27, 0.271, 0.272, 0.273, 0.274, 0.275, 0.276, 0.277, 0.278, 0.279, 0.28, 0.281, 0.28200000000000003, 0.283, 0.284, 0.285, 0.286, 0.287, 0.288, 0.289, 0.29, 0.291, 0.292, 0.293, 0.294, 0.295, 0.296, 0.297, 0.298, 0.299, 0.3, 0.301, 0.302, 0.303, 0.304, 0.305, 0.306, 0.307, 0.308, 0.309, 0.31, 0.311, 0.312, 0.313, 0.314, 0.315, 0.316, 0.317, 0.318, 0.319, 0.32, 0.321, 0.322, 0.323, 0.324, 0.325, 0.326, 0.327, 0.328, 0.329, 0.33, 0.331, 0.332, 0.333, 0.334, 0.335, 0.336, 0.337, 0.338, 0.339, 0.34, 0.341, 0.342, 0.343, 0.34400000000000003, 0.345, 0.346, 0.347, 0.348, 0.349, 0.35, 0.351, 0.352, 0.353, 0.354, 0.355, 0.356, 0.357, 0.358, 0.359, 0.36, 0.361, 0.362, 0.363, 0.364, 0.365, 0.366, 0.367, 0.368, 0.369, 0.37, 0.371, 0.372, 0.373, 0.374, 0.375, 0.376, 0.377, 0.378, 0.379, 0.38, 0.381, 0.382, 0.383, 0.384, 0.385, 0.386, 0.387, 0.388, 0.389, 0.39, 0.391, 0.392, 0.393, 0.394, 0.395, 0.396, 0.397, 0.398, 0.399, 0.4, 0.401, 0.402, 0.403, 0.404, 0.405, 0.406, 0.40700000000000003, 0.408, 0.409, 0.41, 0.411, 0.412, 0.413, 0.414, 0.415, 0.416, 0.417, 0.418, 0.419, 0.42, 0.421, 0.422, 0.423, 0.424, 0.425, 0.426, 0.427, 0.428, 0.429, 0.43, 0.431, 0.432, 0.433, 0.434, 0.435, 0.436, 0.437, 0.438, 0.439, 0.44, 0.441, 0.442, 0.443, 0.444, 0.445, 0.446, 0.447, 0.448, 0.449, 0.45, 0.451, 0.452, 0.453, 0.454, 0.455, 0.456, 0.457, 0.458, 0.459, 0.46, 0.461, 0.462, 0.463, 0.464, 0.465, 0.466, 0.467, 0.468, 0.46900000000000003, 0.47, 0.471, 0.472, 0.473, 0.474, 0.475, 0.476, 0.477, 0.478, 0.479, 0.48, 0.481, 0.482, 0.483, 0.484, 0.485, 0.486, 0.487, 0.488, 0.489, 0.49, 0.491, 0.492, 0.493, 0.494, 0.495, 0.496, 0.497, 0.498, 0.499, 0.5, 0.501, 0.502, 0.503, 0.504, 0.505, 0.506, 0.507, 0.508, 0.509, 0.51, 0.511, 0.512, 0.513, 0.514, 0.515, 0.516, 0.517, 0.518, 0.519, 0.52, 0.521, 0.522, 0.523, 0.524, 0.525, 0.526, 0.527, 0.528, 0.529, 0.53, 0.531, 0.532, 0.533, 0.534, 0.535, 0.536, 0.537, 0.538, 0.539, 0.54, 0.541, 0.542, 0.543, 0.544, 0.545, 0.546, 0.547, 0.548, 0.549, 0.55, 0.551, 0.552, 0.553, 0.554, 0.555, 0.556, 0.557, 0.558, 0.559, 0.56, 0.561, 0.562, 0.5630000000000001, 0.564, 0.565, 0.566, 0.567, 0.568, 0.569, 0.57, 0.571, 0.572, 0.573, 0.574, 0.575, 0.576, 0.577, 0.578, 0.579, 0.58, 0.581, 0.582, 0.583, 0.584, 0.585, 0.586, 0.587, 0.588, 0.589, 0.59, 0.591, 0.592, 0.593, 0.594, 0.595, 0.596, 0.597, 0.598, 0.599, 0.6, 0.601, 0.602, 0.603, 0.604, 0.605, 0.606, 0.607, 0.608, 0.609, 0.61, 0.611, 0.612, 0.613, 0.614, 0.615, 0.616, 0.617, 0.618, 0.619, 0.62, 0.621, 0.622, 0.623, 0.624, 0.625, 0.626, 0.627, 0.628, 0.629, 0.63, 0.631, 0.632, 0.633, 0.634, 0.635, 0.636, 0.637, 0.638, 0.639, 0.64, 0.641, 0.642, 0.643, 0.644, 0.645, 0.646, 0.647, 0.648, 0.649, 0.65, 0.651, 0.652, 0.653, 0.654, 0.655, 0.656, 0.657, 0.658, 0.659, 0.66, 0.661, 0.662, 0.663, 0.664, 0.665, 0.666, 0.667, 0.668, 0.669, 0.67, 0.671, 0.672, 0.673, 0.674, 0.675, 0.676, 0.677, 0.678, 0.679, 0.68, 0.681, 0.682, 0.683, 0.684, 0.685, 0.686, 0.687, 0.6880000000000001, 0.689, 0.69, 0.691, 0.692, 0.693, 0.694, 0.695, 0.696, 0.697, 0.698, 0.699, 0.7, 0.701, 0.702, 0.703, 0.704, 0.705, 0.706, 0.707, 0.708, 0.709, 0.71, 0.711, 0.712, 0.713, 0.714, 0.715, 0.716, 0.717, 0.718, 0.719, 0.72, 0.721, 0.722, 0.723, 0.724, 0.725, 0.726, 0.727, 0.728, 0.729, 0.73, 0.731, 0.732, 0.733, 0.734, 0.735, 0.736, 0.737, 0.738, 0.739, 0.74, 0.741, 0.742, 0.743, 0.744, 0.745, 0.746, 0.747, 0.748, 0.749, 0.75, 0.751, 0.752, 0.753, 0.754, 0.755, 0.756, 0.757, 0.758, 0.759, 0.76, 0.761, 0.762, 0.763, 0.764, 0.765, 0.766, 0.767, 0.768, 0.769, 0.77, 0.771, 0.772, 0.773, 0.774, 0.775, 0.776, 0.777, 0.778, 0.779, 0.78, 0.781, 0.782, 0.783, 0.784, 0.785, 0.786, 0.787, 0.788, 0.789, 0.79, 0.791, 0.792, 0.793, 0.794, 0.795, 0.796, 0.797, 0.798, 0.799, 0.8, 0.801, 0.802, 0.803, 0.804, 0.805, 0.806, 0.807, 0.808, 0.809, 0.81, 0.811, 0.812, 0.8130000000000001, 0.814, 0.815, 0.816, 0.817, 0.818, 0.819, 0.82, 0.821, 0.822, 0.823, 0.824, 0.825, 0.826, 0.827, 0.828, 0.829, 0.83, 0.831, 0.832, 0.833, 0.834, 0.835, 0.836, 0.837, 0.838, 0.839, 0.84, 0.841, 0.842, 0.843, 0.844, 0.845, 0.846, 0.847, 0.848, 0.849, 0.85, 0.851, 0.852, 0.853, 0.854, 0.855, 0.856, 0.857, 0.858, 0.859, 0.86, 0.861, 0.862, 0.863, 0.864, 0.865, 0.866, 0.867, 0.868, 0.869, 0.87, 0.871, 0.872, 0.873, 0.874, 0.875, 0.876, 0.877, 0.878, 0.879, 0.88, 0.881, 0.882, 0.883, 0.884, 0.885, 0.886, 0.887, 0.888, 0.889, 0.89, 0.891, 0.892, 0.893, 0.894, 0.895, 0.896, 0.897, 0.898, 0.899, 0.9, 0.901, 0.902, 0.903, 0.904, 0.905, 0.906, 0.907, 0.908, 0.909, 0.91, 0.911, 0.912, 0.913, 0.914, 0.915, 0.916, 0.917, 0.918, 0.919, 0.92, 0.921, 0.922, 0.923, 0.924, 0.925, 0.926, 0.927, 0.928, 0.929, 0.93, 0.931, 0.932, 0.933, 0.934, 0.935, 0.936, 0.937, 0.9380000000000001, 0.939, 0.94, 0.941, 0.942, 0.943, 0.944, 0.945, 0.946, 0.947, 0.948, 0.949, 0.95, 0.951, 0.952, 0.953, 0.954, 0.955, 0.956, 0.957, 0.958, 0.959, 0.96, 0.961, 0.962, 0.963, 0.964, 0.965, 0.966, 0.967, 0.968, 0.969, 0.97, 0.971, 0.972, 0.973, 0.974, 0.975, 0.976, 0.977, 0.978, 0.979, 0.98, 0.981, 0.982, 0.983, 0.984, 0.985, 0.986, 0.987, 0.988, 0.989, 0.99, 0.991, 0.992, 0.993, 0.994, 0.995, 0.996, 0.997, 0.998, 0.999, 1.0])\n"
     ]
    }
   ],
   "source": [
    "for param in parameters.items():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13634cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.501}\n",
      "486\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Extract the keys and values from the parameters dictionary\n",
    "keys = parameters.keys()\n",
    "values = parameters.values()\n",
    "\n",
    "# Generate the Cartesian product of the parameter values\n",
    "combinations = list(itertools.product(*values))\n",
    "\n",
    "# Convert each combination into a dictionary\n",
    "combinations_dicts = [dict(zip(keys, combo)) for combo in combinations]\n",
    "\n",
    "# Example to print the first few combinations\n",
    "for combo in combinations_dicts[:5]:\n",
    "    print(combo)\n",
    "\n",
    "print(len(combinations_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30c1ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.901}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.101}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.201}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.301}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.401}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.501}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.601}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.701}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.801}\n",
      "{'init': 'he_normal', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.1, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.2, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.3, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.4, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.5, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.6, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.7, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.8, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.2, 'adam_beta1': 0.9, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.1, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.2, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.3, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.4, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.5, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.6, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.7, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.8, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.3, 'adam_beta1': 0.9, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.1, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.2, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.3, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.4, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.5, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.6, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.7, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.8, 'adam_beta2': 0.901}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.101}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.201}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.301}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.401}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.501}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.601}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.701}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.801}\n",
      "{'init': 'glorot_uniform', 'dropout_rate': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.901}\n"
     ]
    }
   ],
   "source": [
    "for combo in combinations_dicts:\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015f5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
